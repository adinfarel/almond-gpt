training:
  vocab_size: 8192
  block_size: 128
  n_layers: 8
  n_heads: 12
  n_embd: 768
  dropout: 0.1
  device: "cpu"
  learning_rate: 0.0003
  batch_size: 64
  training: true

eval:
  max_iters: 5000
  eval_interval: 500
  eval_iters: 200

path:
  text_bin_path: data/processed/processed_data.bin
  model_save_path: models/checkpoints/gpt_model.pt